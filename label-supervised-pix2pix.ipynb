{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pL_qh-9CwifL","executionInfo":{"status":"ok","timestamp":1606775497374,"user_tz":300,"elapsed":1611,"user":{"displayName":"Sketch Pictures","photoUrl":"","userId":"16800295059730283832"}},"outputId":"bd4ff298-28d1-4f47-9549-5962c1678f6c"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon Nov 30 22:31:38 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ajt9cNQuxBG3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607197031554,"user_tz":300,"elapsed":22457,"user":{"displayName":"Sketch Pictures","photoUrl":"","userId":"16800295059730283832"}},"outputId":"a02c0a2e-0489-4a62-eaac-c95e0148664f"},"source":["# Reference: https://zhangruochi.com/Pix2Pix/2020/11/09/, \n","# https://github.com/kewellcjj/pytorch-multiple-style-transfer/blob/8a78000360cc36d3bbabdf838b91b6d12f88ae82/transformer_net.py\n","# https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4 \n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b27IAMVZvjFk","executionInfo":{"status":"ok","timestamp":1607197031555,"user_tz":300,"elapsed":8688,"user":{"displayName":"Sketch Pictures","photoUrl":"","userId":"16800295059730283832"}},"outputId":"42c0db10-5f2d-4de1-d0d7-560cecdeef3b"},"source":["cd gdrive/MyDrive/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LlhLZWqWwZxw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607197031556,"user_tz":300,"elapsed":8459,"user":{"displayName":"Sketch Pictures","photoUrl":"","userId":"16800295059730283832"}},"outputId":"7b768ef8-3b26-4f0a-ce02-71bbec5c8389"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" \u001b[0m\u001b[01;34mArchive\u001b[0m/\n"," \u001b[01;34mclassifier\u001b[0m/\n"," Classifier.ipynb\n"," \u001b[01;34mData\u001b[0m/\n","\u001b[01;34m'Data processing'\u001b[0m/\n"," edges2shoes.tar.gz\n"," pix2pix_11000.pth\n"," pix2pix_13750.pth\n"," pix2pix_16500.pth\n"," pix2pix_19250.pth\n"," pix2pix_22000.pth\n"," pix2pix_24750.pth\n"," pix2pix_27500.pth\n"," pix2pix_2750.pth\n"," pix2pix_5500.pth\n"," pix2pix_8250.pth\n"," pix2pixlabelsupervised_11000.pth\n"," pix2pixlabelsupervised_13750.pth\n"," pix2pixlabelsupervised_16500.pth\n"," pix2pixlabelsupervised_19250.pth\n"," pix2pixlabelsupervised_22000.pth\n"," pix2pixlabelsupervised_24750.pth\n"," pix2pixlabelsupervised_27500.pth\n"," pix2pixlabelsupervised_2750.pth\n"," pix2pixlabelsupervised_5500.pth\n"," pix2pixlabelsupervised_8250.pth\n"," pix2pixlabelsupervisedprogressivetraining_11000.pth\n"," pix2pixlabelsupervisedprogressivetraining_13750.pth\n"," pix2pixlabelsupervisedprogressivetraining_16500.pth\n"," pix2pixlabelsupervisedprogressivetraining_19250.pth\n"," pix2pixlabelsupervisedprogressivetraining_22000.pth\n"," pix2pixlabelsupervisedprogressivetraining_24750.pth\n"," pix2pixlabelsupervisedprogressivetraining_27500.pth\n"," pix2pixlabelsupervisedprogressivetraining_2750.pth\n"," pix2pixlabelsupervisedprogressivetraining_30250.pth\n"," pix2pixlabelsupervisedprogressivetraining_33000.pth\n"," pix2pixlabelsupervisedprogressivetraining_5500.pth\n"," pix2pixlabelsupervisedprogressivetraining_8250.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_10000.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_15000.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_20000.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_25000.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_30000.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_35000.pth\n"," pix2pixlabelsupervisedwithnoiseinjection_5000.pth\n"," pix2pixlabelsupervisedwithwgangp_10000.pth\n"," pix2pixlabelsupervisedwithwgangp_11000.pth\n"," pix2pixlabelsupervisedwithwgangp_13750.pth\n"," pix2pixlabelsupervisedwithwgangp_15000.pth\n"," pix2pixlabelsupervisedwithwgangp_16500.pth\n"," pix2pixlabelsupervisedwithwgangp_19250.pth\n"," pix2pixlabelsupervisedwithwgangp_20000.pth\n"," pix2pixlabelsupervisedwithwgangp_22000.pth\n"," pix2pixlabelsupervisedwithwgangp_24750.pth\n"," pix2pixlabelsupervisedwithwgangp_25000.pth\n"," pix2pixlabelsupervisedwithwgangp_27500.pth\n"," pix2pixlabelsupervisedwithwgangp_2750.pth\n"," pix2pixlabelsupervisedwithwgangp_30000.pth\n"," pix2pixlabelsupervisedwithwgangp_35000.pth\n"," pix2pixlabelsupervisedwithwgangp_5000.pth\n"," pix2pixlabelsupervisedwithwgangp_5500.pth\n"," pix2pixlabelsupervisedwithwgangp_8250.pth\n"," \u001b[01;34mPresentation\u001b[0m/\n"," \u001b[01;34mpytoch-pix2pix\u001b[0m/\n"," \u001b[01;34mpytorch-hed\u001b[0m/\n"," \u001b[01;34mSTL-10-database\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q8fKV2wGxTM2"},"source":["import torch\n","import torchvision\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split, ConcatDataset\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","torch.manual_seed(0)\n","\n","def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n","    '''\n","    Function for visualizing images: Given a tensor of images, number of images, and\n","    size per image, plots and prints the images in an uniform grid.\n","    '''\n","    image_shifted = image_tensor\n","    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n","    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n","    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKlfvV3vcnQ_"},"source":["U-Net code"]},{"cell_type":"code","metadata":{"id":"APfp2UG7xTj2"},"source":["def crop(image, new_shape):\n","    '''\n","    Function for cropping an image tensor: Given an image tensor and the new shape,\n","    crops to the center pixels.\n","    Parameters:\n","        image: image tensor of shape (batch size, channels, height, width)\n","        new_shape: a torch.Size object with the shape you want x to have\n","    '''\n","    middle_height = image.shape[2] // 2\n","    middle_width = image.shape[3] // 2\n","    starting_height = middle_height - round(new_shape[2] / 2)\n","    final_height = starting_height + new_shape[2]\n","    starting_width = middle_width - round(new_shape[3] / 2)\n","    final_width = starting_width + new_shape[3]\n","    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n","    return cropped_image\n","\n","class conditionalInstanceNorm2d(torch.nn.Module):\n","    \"\"\"\n","    Conditional Instance Normalization\n","    introduced in https://arxiv.org/abs/1610.07629\n","    created and applied based on my limited understanding, could be improved\n","    \"\"\"\n","    def __init__(self, style_num, in_channels):\n","        super(conditionalInstanceNorm2d, self).__init__()\n","        self.inns = torch.nn.ModuleList([torch.nn.InstanceNorm2d(in_channels, affine=True) for i in range(style_num)])\n","\n","    def forward(self, x, style_id):\n","        # print(len(style_id))\n","        # print(style_id.size())\n","        # print(style_id)\n","        out = torch.stack([self.inns[style_id[i]](x[i].unsqueeze(0)).squeeze_(0) for i in range(len(style_id))])\n","        return out\n","  \n","class ContractingBlock(nn.Module):\n","    '''\n","    ContractingBlock Class\n","    Performs two convolutions followed by a max pool operation.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n","        super(ContractingBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(input_channels * 2, input_channels * 2, kernel_size=3, padding=1)\n","        self.activation = nn.LeakyReLU(0.2)\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        if use_bn:\n","            self.batchnorm = nn.BatchNorm2d(input_channels * 2)\n","        self.use_bn = use_bn\n","        if use_dropout:\n","            self.dropout = nn.Dropout()\n","        self.use_dropout = use_dropout\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ContractingBlock: \n","        Given an image tensor, completes a contracting block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv1(x)\n","        if self.use_bn:\n","            x = self.batchnorm(x)\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        x = self.activation(x)\n","        x = self.conv2(x)\n","        if self.use_bn:\n","            x = self.batchnorm(x)\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        x = self.activation(x)\n","        x = self.maxpool(x)\n","        return x\n","\n","class ExpandingBlock(nn.Module):\n","    '''\n","    ExpandingBlock Class:\n","    Performs an upsampling, a convolution, a concatenation of its two inputs,\n","    followed by two more convolutions with optional dropout\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n","        super(ExpandingBlock, self).__init__()\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        self.conv1 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=2)\n","        self.conv2 = nn.Conv2d(input_channels, input_channels // 2, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(input_channels // 2, input_channels // 2, kernel_size=2, padding=1)\n","        if use_bn:\n","            self.batchnorm = nn.BatchNorm2d(input_channels // 2)\n","        self.use_bn = use_bn\n","        self.activation = nn.ReLU()\n","        if use_dropout:\n","            self.dropout = nn.Dropout()\n","        self.use_dropout = use_dropout\n","\n","    def forward(self, x, skip_con_x):\n","        '''\n","        Function for completing a forward pass of ExpandingBlock: \n","        Given an image tensor, completes an expanding block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n","                    for the skip connection\n","        '''\n","        x = self.upsample(x)\n","        x = self.conv1(x)\n","        skip_con_x = crop(skip_con_x, x.shape)\n","        x = torch.cat([x, skip_con_x], axis=1)\n","        x = self.conv2(x)\n","        if self.use_bn:\n","            x = self.batchnorm(x)\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        x = self.activation(x)\n","        x = self.conv3(x)\n","        if self.use_bn:\n","            x = self.batchnorm(x)\n","        if self.use_dropout:\n","            x = self.dropout(x)\n","        x = self.activation(x)\n","        return x\n","\n","class FeatureMapBlock(nn.Module):\n","    '''\n","    FeatureMapBlock Class\n","    The final layer of a U-Net - \n","    maps each pixel to a pixel with the correct number of output dimensions\n","    using a 1x1 convolution.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels):\n","        super(FeatureMapBlock, self).__init__()\n","        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of FeatureMapBlock: \n","        Given an image tensor, returns it mapped to the desired number of channels.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv(x)\n","        return x\n","\n","class UNet(nn.Module):\n","    '''\n","    UNet Class\n","    A series of 4 contracting blocks followed by 4 expanding blocks to \n","    transform an input image into the corresponding paired image, with an upfeature\n","    layer at the start and a downfeature layer at the end.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels, hidden_channels=32, style_num=5):\n","        super(UNet, self).__init__()\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.in1 = conditionalInstanceNorm2d(style_num, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels, use_dropout=True)\n","        self.in2 = conditionalInstanceNorm2d(style_num, hidden_channels * 2)\n","        self.contract2 = ContractingBlock(hidden_channels * 2, use_dropout=True)\n","        self.in3 = conditionalInstanceNorm2d(style_num, hidden_channels * 4)\n","        self.contract3 = ContractingBlock(hidden_channels * 4, use_dropout=True)\n","        self.in4 = conditionalInstanceNorm2d(style_num, hidden_channels * 8)\n","        self.contract4 = ContractingBlock(hidden_channels * 8)\n","        self.in5 = conditionalInstanceNorm2d(style_num, hidden_channels * 16)\n","        self.contract5 = ContractingBlock(hidden_channels * 16)\n","        self.in6 = conditionalInstanceNorm2d(style_num, hidden_channels * 32)\n","        # self.contract6 = ContractingBlock(hidden_channels * 32)\n","        # self.in7 = conditionalInstanceNorm2d(style_num, hidden_channels * 64)\n","        # self.expand0 = ExpandingBlock(hidden_channels * 64)\n","        # self.in7 = conditionalInstanceNorm2d(style_num, hidden_channels * 32)\n","        self.expand1 = ExpandingBlock(hidden_channels * 32)\n","        self.in7 = conditionalInstanceNorm2d(style_num, hidden_channels * 16)\n","        self.expand2 = ExpandingBlock(hidden_channels * 16)\n","        self.in8 = conditionalInstanceNorm2d(style_num, hidden_channels * 8)\n","        self.expand3 = ExpandingBlock(hidden_channels * 8)\n","        self.in9 = conditionalInstanceNorm2d(style_num, hidden_channels * 4)\n","        self.expand4 = ExpandingBlock(hidden_channels * 4)\n","        self.in10 = conditionalInstanceNorm2d(style_num, hidden_channels * 2)\n","        self.expand5 = ExpandingBlock(hidden_channels * 2)\n","        self.in11 = conditionalInstanceNorm2d(style_num, hidden_channels)\n","        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n","        self.in12 = conditionalInstanceNorm2d(style_num, output_channels)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x, style_id=0):\n","        '''\n","        Function for completing a forward pass of UNet: \n","        Given an image tensor, passes it through U-Net and returns the output.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x0 = self.in1(self.upfeature(x), style_id)\n","        x1 = self.in2(self.contract1(x0), style_id)\n","        x2 = self.in3(self.contract2(x1), style_id)\n","        x3 = self.in4(self.contract3(x2), style_id)\n","        x4 = self.in5(self.contract4(x3), style_id)\n","        x5 = self.in6(self.contract5(x4), style_id)\n","        # x6 = self.in7(self.contract6(x5), style_id)\n","        # x7 = self.in8(self.expand0(x6, x5), style_id)\n","        x6 = self.in7(self.expand1(x5, x4), style_id)\n","        x7 = self.in8(self.expand2(x6, x3), style_id)\n","        x8 = self.in9(self.expand3(x7, x2), style_id)\n","        x9 = self.in10(self.expand4(x8, x1), style_id)\n","        x10 = self.in11(self.expand5(x9, x0), style_id)\n","        xn = self.in12(self.downfeature(x10), style_id)\n","        return self.sigmoid(xn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVJdnZEeef9R"},"source":["Patch GAN discriminator\n"]},{"cell_type":"code","metadata":{"id":"4SkS9Y-vxmO7"},"source":["# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED CLASS: Discriminator\n","class Discriminator(nn.Module):\n","    '''\n","    Discriminator Class\n","    Structured like the contracting path of the U-Net, the discriminator will\n","    output a matrix of values classifying corresponding portions of the image as real or fake. \n","    Parameters:\n","        input_channels: the number of image input channels\n","        hidden_channels: the initial number of discriminator convolutional filters\n","    '''\n","    def __init__(self, input_channels, hidden_channels=8):\n","        super(Discriminator, self).__init__()\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels, use_bn=False)\n","        self.contract2 = ContractingBlock(hidden_channels * 2)\n","        self.contract3 = ContractingBlock(hidden_channels * 4)\n","        self.contract4 = ContractingBlock(hidden_channels * 8)\n","        self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)\n","        self.fc1 = nn.Linear(36, 5)\n","\n","    def forward(self, x, y):\n","        x = torch.cat([x, y], axis=1)\n","        x0 = self.upfeature(x)\n","        x1 = self.contract1(x0)\n","        x2 = self.contract2(x1)\n","        x3 = self.contract3(x2)\n","        x4 = self.contract4(x3)\n","        xn = self.final(x4)\n","        # print(c.size())\n","        c = xn.view(-1, 36)\n","        # print(c.size())\n","        c = torch.sigmoid(self.fc1(c))\n","        return xn, c\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V62vUT7bxE6f"},"source":["Training Preparation"]},{"cell_type":"code","metadata":{"id":"ns5O8UF8xm9V"},"source":["import torch.nn.functional as F\n","# New parameters\n","adv_criterion = nn.BCEWithLogitsLoss() \n","recon_criterion = nn.L1Loss() \n","lambda_recon = 200\n","\n","n_epochs = 50\n","input_dim = 3\n","real_dim = 3\n","display_step = 200\n","batch_size = 4\n","lr = 0.0002\n","target_shape = 96\n","device = 'cuda'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mi4KeZTjiL-y"},"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","import torchvision\n","stl10data =  torchvision.datasets.ImageFolder(\"Data/STL-10/Final Concatenated/\", transform=transform)\n","sketchydata = torchvision.datasets.ImageFolder(\"Data/sketchy-database/Final Concatenated/\", transform=transform)\n","stl10train, stl10val = random_split(stl10data, [2200, 175], generator=torch.Generator().manual_seed(0))\n","sketchytrain, sketchyval, testData = random_split(sketchydata, [2200, 175, 125], generator=torch.Generator().manual_seed(0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68PQXK3nu-7n"},"source":["gen = UNet(input_dim, real_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n","disc = Discriminator(input_dim + real_dim).to(device)\n","disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)\n","\n","pretrained = True\n","if pretrained:\n","    loaded_state = torch.load(\"pix2pixlabelsupervised_27500.pth\")\n","    gen.load_state_dict(loaded_state[\"gen\"])\n","    gen_opt.load_state_dict(loaded_state[\"gen_opt\"])\n","    disc.load_state_dict(loaded_state[\"disc\"])\n","    disc_opt.load_state_dict(loaded_state[\"disc_opt\"])\n","else:\n","    gen = gen.apply(weights_init)\n","    disc = disc.apply(weights_init)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnSNSZCAJn77"},"source":["    from torchvision.utils import save_image\n","    pt = 2750\n","    while pt <= 27500:\n","      loaded_state = torch.load(\"pix2pixlabelsupervised_\"+str(pt)+\".pth\")\n","      gen.load_state_dict(loaded_state[\"gen\"])\n","      gen_opt.load_state_dict(loaded_state[\"gen_opt\"])\n","      disc.load_state_dict(loaded_state[\"disc\"])\n","      disc_opt.load_state_dict(loaded_state[\"disc_opt\"])\n","      valIndices = list(range(40,60,1))\n","      dset = torch.utils.data.Subset(stl10val, valIndices)\n","      sampleDataLoader = DataLoader(dset)\n","      count = 6\n","      for image, label in sampleDataLoader:\n","        image_width = image.shape[3]\n","        image = image.to(device)\n","        condition = image[:, :, :, :image_width // 2]\n","        condition = nn.functional.interpolate(condition, size=target_shape)\n","        real = image[:, :, :, image_width // 2:]\n","        real = nn.functional.interpolate(real, size=target_shape)\n","        cur_batch_size = len(condition)\n","        condition = condition.to(device)\n","        real = real.to(device)\n","\n","        with torch.no_grad():\n","           fake = gen(condition, style_id = label)\n","        fake = F.interpolate(fake, size=96)\n","        result = torch.cat((image, fake),3).to(device)\n","        save_image(result,'pytoch-pix2pix/val-result-pix2pix-label-supervised/'+str(pt)+'_'+str(count)+'.png')\n","        count += 1\n","      pt += 2750"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJP_wpIiwW3l"},"source":["def get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon, label):\n","    '''\n","    Return the loss of the generator given inputs.\n","    Parameters:\n","        gen: the generator; takes the condition and returns potential images\n","        disc: the discriminator; takes images and the condition and\n","          returns real/fake prediction matrices\n","        real: the real images (e.g. maps) to be used to evaluate the reconstruction\n","        condition: the source images (e.g. satellite imagery) which are used to produce the real images\n","        adv_criterion: the adversarial loss function; takes the discriminator \n","                  predictions and the true labels and returns a adversarial \n","                  loss (which you aim to minimize)\n","        recon_criterion: the reconstruction loss function; takes the generator \n","                    outputs and the real images and returns a reconstructuion \n","                    loss (which you aim to minimize)\n","        lambda_recon: the degree to which the reconstruction loss should be weighted in the sum\n","    '''\n","    # Steps: 1) Generate the fake images, based on the conditions.\n","    #        2) Evaluate the fake images and the condition with the discriminator.\n","    #        3) Calculate the adversarial and reconstruction losses.\n","    #        4) Add the two losses, weighting the reconstruction loss appropriately.\n","    gen_img = gen(condition, style_id = label)\n","    out, c = disc(gen_img, condition)\n","    adv_loss = adv_criterion(out, torch.ones_like(out))\n","    recon_loss = recon_criterion(gen_img, real)\n","    gen_loss = adv_loss + lambda_recon * recon_loss\n","    return gen_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b1w6iR5sxBrH"},"source":["Pix2Pix Training"]},{"cell_type":"code","metadata":{"id":"JpGtUsFOdbW3"},"source":["from skimage import color\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torchvision.utils import save_image\n","\n","\n","def train(save_model=False):\n","    mean_generator_loss = 0\n","    mean_discriminator_loss = 0\n","    cur_step = 0\n","    curStepCount = []\n","    genLossCount = []\n","    disLossCount = []\n","    accCount = []\n","    valAccCount = []\n","    epochCount = []\n","    dataloader = DataLoader(stl10train, batch_size=batch_size, shuffle=True)\n","    valDataLoader = DataLoader(stl10val, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(n_epochs):\n","\n","        epochCount += [epoch]\n","        acc = 0\n","        # Dataloader returns the batches\n","        for image, label in tqdm(dataloader):\n","            image_width = image.shape[3]\n","            condition = image[:, :, :, :image_width // 2]\n","            condition = nn.functional.interpolate(condition, size=target_shape)\n","            real = image[:, :, :, image_width // 2:]\n","            real = nn.functional.interpolate(real, size=target_shape)\n","            cur_batch_size = len(condition)\n","            condition = condition.to(device)\n","            real = real.to(device)\n","\n","            labels = torch.zeros(batch_size,5).float().to(device)\n","            for j in range(batch_size):\n","              val = label[j].item()\n","              labels[j][val] =1.0 \n","\n","            ### Update discriminator ###\n","            disc_opt.zero_grad() # Zero out the gradient before backpropagation\n","            with torch.no_grad():\n","                print(label)\n","                print(condition)\n","                # print(a)\n","                fake = gen(condition, style_id = label)\n","            disc_fake_hat, c_fake = disc(fake.detach(), condition) # Detach generator\n","            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n","            # print(disc_fake_hat.size())\n","            # print(c_fake)-\n","            # print(c_fake.size())\n","            # print(labels.size())\n","            # print(labels)\n","            disc_fake_class_loss = adv_criterion(c_fake, labels)\n","            disc_real_hat, c_real = disc(real, condition)\n","            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n","            disc_real_class_loss = adv_criterion(c_real, labels)\n","            disc_loss = (disc_fake_loss + disc_real_loss + disc_fake_class_loss + disc_real_class_loss) / 4\n","            disc_loss.backward(retain_graph=True) # Update gradients\n","            disc_opt.step() # Update optimizer\n","\n","            ### Update generator ###\n","            gen_opt.zero_grad()\n","            gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon, label)\n","            gen_loss.backward() # Update gradients\n","            gen_opt.step() # Update optimizer\n","\n","            ### Evaluate discriminator accuracy\n","            for i in torch.mean(disc_real_hat,[1,2,3]):\n","              if i > 0.5:\n","                acc += 1\n","            for i in torch.mean(disc_fake_hat,[1,2,3]):\n","              if i < 0.5:\n","                acc += 1\n","                \n","            # Keep track of the average discriminator loss\n","            mean_discriminator_loss += disc_loss.item() / display_step\n","            # Keep track of the average generator loss\n","            mean_generator_loss += gen_loss.item() / display_step\n","\n","            ### Visualization code ###\n","            if cur_step % display_step == 0:\n","                if cur_step > 0:\n","                    print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) train loss: {mean_generator_loss}, Discriminator train loss: {mean_discriminator_loss}\")\n","                else:\n","                    print(\"Pretrained initial state\")\n","                show_tensor_images(condition, size=(input_dim, target_shape, target_shape))\n","                show_tensor_images(real, size=(real_dim, target_shape, target_shape))\n","                show_tensor_images(fake, size=(real_dim, target_shape, target_shape))\n","                genLossCount += [mean_generator_loss]\n","                disLossCount += [mean_discriminator_loss]\n","                curStepCount += [cur_step]          \n","                mean_generator_loss = 0\n","                mean_discriminator_loss = 0\n","            cur_step += 1\n","\n","        accCount += [acc/4400]\n","        print(f\"D training acc: {acc / 4400}\")\n","        acc = 0\n","        with torch.no_grad():\n","          for valImage, valLabel in valDataLoader:\n","                    valCondition = valImage[:, :, :, :image_width // 2]\n","                    valCondition = nn.functional.interpolate(valCondition, size=target_shape)\n","                    valReal = valImage[:, :, :, image_width // 2:]\n","                    valReal = nn.functional.interpolate(valReal, size=target_shape)\n","                    cur_batch_size = len(valCondition)\n","                    valCondition = valCondition.to(device)\n","                    valReal = valReal.to(device)\n","\n","                    ### Update discriminator ###\n","                    valFake = gen(valCondition, style_id = valLabel)\n","                    valdisc_fake_hat, _ = disc(valFake.detach(), valCondition) # Detach generator\n","                    valdisc_real_hat, _ = disc(valReal, valCondition)\n","\n","                    ### Evaluate discriminator accuracy\n","                    for i in torch.mean(valdisc_fake_hat,[1,2,3]):\n","                      if i < 0.5:\n","                        acc += 1\n","                    for i in torch.mean(valdisc_real_hat,[1,2,3]):\n","                      if i > 0.5:\n","                        acc += 1\n","        print(f\"D val acc: {acc / 350}\")\n","        valAccCount += [acc/350]\n","        if epoch % 5 == 4:\n","          if epoch > 0:\n","            if save_model:\n","              torch.save({'gen': gen.state_dict(),\n","                          'gen_opt': gen_opt.state_dict(),\n","                          'disc': disc.state_dict(),\n","                          'disc_opt': disc_opt.state_dict()\n","                          }, f\"pix2pixlabelsupervised_{cur_step}.pth\")\n","              print('saved '+str(cur_step)+'.pth')\n","          valIndices = [0,5,22,41,53]\n","          dset = torch.utils.data.Subset(stl10val, valIndices)\n","          sampleDataLoader = DataLoader(dset)\n","          count = 1\n","          for image, label in sampleDataLoader:\n","            image_width = image.shape[3]\n","            image = image.to(device)\n","            condition = image[:, :, :, :image_width // 2]\n","            condition = nn.functional.interpolate(condition, size=target_shape)\n","            real = image[:, :, :, image_width // 2:]\n","            real = nn.functional.interpolate(real, size=target_shape)\n","            cur_batch_size = len(condition)\n","            condition = condition.to(device)\n","            real = real.to(device)\n","\n","            with torch.no_grad():\n","              fake = gen(condition, style_id = label)\n","            fake = F.interpolate(fake, size=96)\n","            result = torch.cat((image, fake),3).to(device)\n","            save_image(result,'pytoch-pix2pix/val-result-pix2pix-label-supervised/'+str(cur_step)+str(count)+'.png')\n","            count += 1\n","    plt.title(\"Generator Loss Curve\")\n","    plt.plot(curStepCount, genLossCount, label = \"Training\")\n","    # plt.plot(curStepCount, valGenLossCount, label = \"Validation\")\n","    plt.xlabel(\"Number of Steps\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Discriminator Loss Curve\")\n","    plt.plot(curStepCount, disLossCount, label = \"Training\")\n","    # plt.plot(curStepCount, valDisLossCount, label = \"Validation\")\n","    plt.xlabel(\"Number of Steps\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show() \n","\n","    plt.title(\"Discriminator Accuracy Curve\")\n","    plt.plot(epochCount, accCount, label = \"Training\")\n","    plt.plot(epochCount, valAccCount, label = \"Validation\")\n","    plt.xlabel(\"Number of Steps\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show() \n","  \n","train(save_model = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXyAC0uww2Ck"},"source":["from skimage import color\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torchvision.utils import save_image\n","\n","\n","def train(save_model=False):\n","    mean_generator_loss = 0\n","    mean_discriminator_loss = 0\n","    cur_step = 0\n","    curStepCount = []\n","    genLossCount = []\n","    disLossCount = []\n","    accCount = []\n","    valAccCount = []\n","    epochCount = []\n","    dataloader = DataLoader(stl10train, batch_size=batch_size, shuffle=True)\n","    valDataLoader = DataLoader(stl10val, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(n_epochs):\n","\n","        epochCount += [epoch]\n","        acc = 0\n","        # Dataloader returns the batches\n","        for image, label in tqdm(dataloader):\n","            image_width = image.shape[3]\n","            condition = image[:, :, :, :image_width // 2]\n","            condition = nn.functional.interpolate(condition, size=target_shape)\n","            real = image[:, :, :, image_width // 2:]\n","            real = nn.functional.interpolate(real, size=target_shape)\n","            cur_batch_size = len(condition)\n","            condition = condition.to(device)\n","            real = real.to(device)\n","\n","            labels = torch.zeros(batch_size,5).float().to(device)\n","            for j in range(batch_size):\n","              val = label[j].item()\n","              labels[j][val] =1.0 \n","\n","            ### Update discriminator ###\n","            disc_opt.zero_grad() # Zero out the gradient before backpropagation\n","            with torch.no_grad():\n","                fake = gen(condition, style_id = label)\n","            disc_fake_hat, c_fake = disc(fake.detach(), condition) # Detach generator\n","            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n","            # print(disc_fake_hat.size())\n","            # print(c_fake)-\n","            # print(c_fake.size())\n","            # print(labels.size())\n","            # print(labels)\n","            disc_fake_class_loss = adv_criterion(c_fake, labels)\n","            disc_real_hat, c_real = disc(real, condition)\n","            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n","            disc_real_class_loss = adv_criterion(c_real, labels)\n","            disc_loss = (disc_fake_loss + disc_real_loss + disc_fake_class_loss + disc_real_class_loss) / 4\n","            disc_loss.backward(retain_graph=True) # Update gradients\n","            disc_opt.step() # Update optimizer\n","\n","            ### Update generator ###\n","            gen_opt.zero_grad()\n","            gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon, label)\n","            gen_loss.backward() # Update gradients\n","            gen_opt.step() # Update optimizer\n","\n","            ### Evaluate discriminator accuracy\n","            for i in torch.mean(disc_real_hat,[1,2,3]):\n","              if i > 0.5:\n","                acc += 1\n","            for i in torch.mean(disc_fake_hat,[1,2,3]):\n","              if i < 0.5:\n","                acc += 1\n","                \n","            # Keep track of the average discriminator loss\n","            mean_discriminator_loss += disc_loss.item() / display_step\n","            # Keep track of the average generator loss\n","            mean_generator_loss += gen_loss.item() / display_step\n","\n","            ### Visualization code ###\n","            if cur_step % display_step == 0:\n","                if cur_step > 0:\n","                    print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) train loss: {mean_generator_loss}, Discriminator train loss: {mean_discriminator_loss}\")\n","                else:\n","                    print(\"Pretrained initial state\")\n","                show_tensor_images(condition, size=(input_dim, target_shape, target_shape))\n","                show_tensor_images(real, size=(real_dim, target_shape, target_shape))\n","                show_tensor_images(fake, size=(real_dim, target_shape, target_shape))\n","                genLossCount += [mean_generator_loss]\n","                disLossCount += [mean_discriminator_loss]\n","                curStepCount += [cur_step]          \n","                mean_generator_loss = 0\n","                mean_discriminator_loss = 0\n","            cur_step += 1\n","\n","        accCount += [acc/4400]\n","        print(f\"D training acc: {acc / 4400}\")\n","        acc = 0\n","        with torch.no_grad():\n","          for valImage, valLabel in valDataLoader:\n","                    valCondition = valImage[:, :, :, :image_width // 2]\n","                    valCondition = nn.functional.interpolate(valCondition, size=target_shape)\n","                    valReal = valImage[:, :, :, image_width // 2:]\n","                    valReal = nn.functional.interpolate(valReal, size=target_shape)\n","                    cur_batch_size = len(valCondition)\n","                    valCondition = valCondition.to(device)\n","                    valReal = valReal.to(device)\n","\n","                    ### Update discriminator ###\n","                    valFake = gen(valCondition, style_id = valLabel)\n","                    valdisc_fake_hat, _ = disc(valFake.detach(), valCondition) # Detach generator\n","                    valdisc_real_hat, _ = disc(valReal, valCondition)\n","\n","                    ### Evaluate discriminator accuracy\n","                    for i in torch.mean(valdisc_fake_hat,[1,2,3]):\n","                      if i < 0.5:\n","                        acc += 1\n","                    for i in torch.mean(valdisc_real_hat,[1,2,3]):\n","                      if i > 0.5:\n","                        acc += 1\n","        print(f\"D val acc: {acc / 350}\")\n","        valAccCount += [acc/350]\n","        if epoch % 5 == 4:\n","          if epoch > 0:\n","            if save_model:\n","              torch.save({'gen': gen.state_dict(),\n","                          'gen_opt': gen_opt.state_dict(),\n","                          'disc': disc.state_dict(),\n","                          'disc_opt': disc_opt.state_dict()\n","                          }, f\"pix2pixlabelsupervised_{cur_step}.pth\")\n","              print('saved '+str(cur_step)+'.pth')\n","          valIndices = [0,5,22,41,53]\n","          dset = torch.utils.data.Subset(stl10val, valIndices)\n","          sampleDataLoader = DataLoader(dset)\n","          count = 1\n","          for image, label in sampleDataLoader:\n","            image_width = image.shape[3]\n","            image = image.to(device)\n","            condition = image[:, :, :, :image_width // 2]\n","            condition = nn.functional.interpolate(condition, size=target_shape)\n","            real = image[:, :, :, image_width // 2:]\n","            real = nn.functional.interpolate(real, size=target_shape)\n","            cur_batch_size = len(condition)\n","            condition = condition.to(device)\n","            real = real.to(device)\n","\n","            with torch.no_grad():\n","              fake = gen(condition, style_id = label)\n","            fake = F.interpolate(fake, size=96)\n","            result = torch.cat((image, fake),3).to(device)\n","            save_image(result,'pytoch-pix2pix/val-result-pix2pix-label-supervised/'+str(cur_step)+str(count)+'.png')\n","            count += 1\n","    plt.title(\"Generator Loss Curve\")\n","    plt.plot(curStepCount, genLossCount, label = \"Training\")\n","    # plt.plot(curStepCount, valGenLossCount, label = \"Validation\")\n","    plt.xlabel(\"Number of Steps\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Discriminator Loss Curve\")\n","    plt.plot(curStepCount, disLossCount, label = \"Training\")\n","    # plt.plot(curStepCount, valDisLossCount, label = \"Validation\")\n","    plt.xlabel(\"Number of Steps\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show() \n","\n","    plt.title(\"Discriminator Accuracy Curve\")\n","    plt.plot(epochCount, accCount, label = \"Training\")\n","    plt.plot(epochCount, valAccCount, label = \"Validation\")\n","    plt.xlabel(\"Number of Steps\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show() \n","  \n","train(save_model = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aH2ktI5BVezX"},"source":["testDataloader = DataLoader(testData)\n","count = 1\n","for image, label in testDataloader:\n","\n","  image_width = image.shape[3]\n","  image = image.to(device)\n","  condition = image[:, :, :, :image_width // 2]\n","  condition = nn.functional.interpolate(condition, size=target_shape)\n","  real = image[:, :, :, image_width // 2:]\n","  real = nn.functional.interpolate(real, size=target_shape)\n","  cur_batch_size = len(condition)\n","  condition = condition.to(device)\n","  real = real.to(device)\n","\n","  ### Update discriminator ###\n","  with torch.no_grad():\n","    fake = gen(condition, style_id = label)\n","  fake = F.interpolate(fake, size=96)\n","  result = torch.cat((image, fake),3).to(device)\n","  save_image(result,'pytoch-pix2pix/new-test-result-label-supervised-pix2pix/label-supervised-pix2pix-result/'+str(count)+'.png')\n","  count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f31EZN2TUYhK"},"source":["\n","count = 1\n","\n","for image, label in valDataLoader:\n","\n","  image_width = image.shape[3]\n","  image = image.to(device)\n","  condition = image[:, :, :, :image_width // 2]\n","  condition = nn.functional.interpolate(condition, size=target_shape)\n","  real = image[:, :, :, image_width // 2:]\n","  real = nn.functional.interpolate(real, size=target_shape)\n","  cur_batch_size = len(condition)\n","  condition = condition.to(device)\n","  real = real.to(device)\n","  ### Update discriminator ###\n","  with torch.no_grad():\n","    fake = gen(condition, style_id = label)\n","  fake = F.interpolate(fake, size=96)\n","  result = torch.cat((image, fake),3).to(device)\n","  save_image(result,'pytoch-pix2pix/new-test-result-label-supervised-pix2pix/label-supervised-pix2pix-result/'+str(count)+'.png')\n","  count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7SbfR8EedPT"},"source":[],"execution_count":null,"outputs":[]}]}